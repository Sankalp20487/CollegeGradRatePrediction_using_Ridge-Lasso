---
title: 'Module 3 Assignment: GLM and Logistic Regression'
author: "Sankalp Biswal"
date: "2024-01-26"
output: html_document
---

# Introduction

In this report we delve into the application of logistic regression---a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). We utilize the College dataset from the ISLR package to predict whether a university is private or public based on various features.

------------------------------------------------------------------------

## 1. Import the dataset and perform Exploratory Data Analysis by using descriptive statistics and plots to describe the dataset.

-   **Importing dataset and libraries**

```{r}
library(ISLR)
library(caret)
library(corrplot)
library(skimr)
library(ggplot2)
library(knitr)
library(kableExtra)
library(pROC)

# Importing dataset
df <- College
```

-   **Performing EDA**
    1.  **Descriptive Statistics**

```{r}
summary_college <- summary(College)
kable(summary_college, 
      format = "html", 
      caption = "Table 1: Summary Statistics of College Dataset") %>%
                         kable_styling(bootstrap_options = c("striped", "condensed"))
```

-   In the above table we get an idea about the central tendency of the dataset.

2.  **Boxplot for Outstate vs Private/Public Universities**

```{r}

ggplot(College, aes(x = Private, y = Outstate, fill = Private)) +
  geom_boxplot() +
  scale_fill_manual(values=c("lightblue", "salmon")) +
  labs(title="Figure 1: Outstate Tuition by Private/Public",
       y="Outstate Tuition Cost",
       caption="Note. The above boxplot represents the distribution of out-of-state tuition costs by private and public universities.") +
  theme_minimal(base_size = 12) +
  theme(
        plot.title = element_text(hjust = 0),
        plot.caption = element_text(hjust = 0, size = 8))
```

**Interpretation for Figure 1:**

-   This boxplot compares the out-of-state tuition fees between private (Yes) and public (No) universities. The boxplot for public universities shows a lower median tuition fee compared to private universities, and the range of tuition fees is also narrower. Private universities not only have a higher median out-of-state tuition fee but also a wider range, indicating more variability in what private universities charge. Additionally, there are outliers for both categories, especially for private universities, suggesting that there are some private institutions with exceptionally high tuition fees.

------------------------------------------------------------------------

3.  **Boxplot for Room and Board vs Private/Public Universities**

```{r}

ggplot(College, aes(x = Private, y = Room.Board, fill = Private)) +
  geom_boxplot() +
  scale_fill_manual(values=c("lightblue", "salmon")) +
  labs(title="Figure 2: Room and Board Cost by Private/Public",
       y="Room and Board Cost",
       caption="Note. The above boxplot shows the comparison of room and board costs between private and public universities.") +
  theme_minimal(base_size = 12) +
  theme(
        plot.title = element_text(hjust = 0),
        plot.caption = element_text(hjust = 0, size = 8))
```

**Interpretation for Figure 2:**

-   This boxplot compares the room and board costs between private (Yes) and public (No) universities. The median room and board cost for private universities is higher than that for public universities. Private universities also show a greater range in costs, and there are several outliers indicating that some private universities have exceptionally high room and board costs. The distribution for public universities is more compact, with fewer outliers, suggesting more consistency in their room and board costs.

------------------------------------------------------------------------

4.  **Boxplot for Personal Spending vs Private/Public Universities**

```{r}
ggplot(College, aes(x = Private, y = Personal, fill = Private)) +
  geom_boxplot() +
  scale_fill_manual(values=c("lightblue", "salmon")) +
  labs(title="Figure 3: Personal Spending by Private/Public",
       y="Personal Spending",
       caption="Note. The above boxplot represents the distribution of personal spending by private and public universities.") +
  theme_minimal(base_size = 12) +
  theme(
        plot.title = element_text(hjust = 0),
        plot.caption = element_text(hjust = 0, size = 8))
```

**Interpretation for Figure 3:**

-   The boxplot presents a comparison of personal spending for students at private and public universities. It appears that personal spending is generally higher for students at private universities, as indicated by both the higher median and the presence of more upper outliers. The spread of personal spending at public universities is narrower with a lower median, suggesting that students at public universities tend to have lower personal expenses. However, there are outliers on both ends for public universities, indicating some students with very high or very low personal spending relative to the majority.

------------------------------------------------------------------------

5.  **Boxplot for Phd vs Private/Public Universities**

```{r}
ggplot(College, aes(x = Private, y = PhD, fill = Private)) +
  geom_boxplot() +
  scale_fill_manual(values=c("lightblue", "salmon")) +
  labs(title="Figure 4: PhD by Private/Public",
       y="Percentage of Faculty with PhD",
       caption="Note. The above boxplot shows the comparison of the percentage of faculty with PhD between private and public universities.") +
  theme_minimal(base_size = 12) +
  theme(
        plot.title = element_text(hjust = 0),
        plot.caption = element_text(hjust = 0, size = 8))
```

**Interpretation for Figure 4:**

-   The boxplot shows the percentage of faculty with PhDs at private and public universities. Both private and public universities have a wide range of percentages of faculty with PhDs, but the median percentage is higher for public universities. There are also a number of outliers for public universities where the percentage of faculty with PhDs is relatively low compared to the median. Private universities show less variability in the lower quartile and have fewer lower outliers, indicating a more consistent level of PhDs among their faculty.

------------------------------------------------------------------------

6.  **Scatterplot for F. Undergraduate vs P.Undergraduate by Private/Public Universities**

```{r}
ggplot(df, aes(x = F.Undergrad, y = P.Undergrad, color = Private)) + 
    geom_point(alpha = 0.5) + 
    labs(title = "Figure 5: F.Undergrad vs P.Undergrad by Private/Public",
         caption="Note. The above scatterplot shows the comparison of P. Undergraduate vs F.Undergraduateby private and public universities.")+ theme_minimal(base_size = 12) +
  theme(
        plot.title = element_text(hjust = 0),
        plot.caption = element_text(hjust = 0, size = 8))
```

**Interpretation for Figure 5:**

-   The scatter plot shows the relationship between the number of full-time undergraduate students (F.Undergrad) and part-time undergraduate students (P.Undergrad), with points colored by whether the university is private (Yes) or public (No). Public universities tend to have a larger number of full-time undergraduates compared to private universities. There are some private universities with a relatively high number of part-time undergraduates, which is not as common among public universities. The plot also shows that there is a group of universities with a large number of full-time undergraduates and a smaller number of part-time undergraduates, common in both private and public institutions. Overall, there doesn't seem to be a strong linear relationship between the number of full-time and part-time undergraduates, and the pattern differs between private and public universities.

------------------------------------------------------------------------

7.  **Correlation matrix**

```{r}
df_numeric <- df[, sapply(df, is.numeric)]

# Compute the correlation matrix
cor_matrix <- cor(df_numeric, use = "complete.obs")  # use="complete.obs" handles missing values by case-wise deletion

# Plotting correlation matrix
corrplot(cor_matrix, 
         method = "circle",
         type = "upper",
         tl.col = "black",
         tl.srt = 40,
         tl.cex = 0.75,
         addrect = 3,
         main= "Figure 6: Correlation Matrix Heatmap") 

# Adding title below the plot
mtext("Figure 6: Correlation Matrix Heatmap for Numeric Variables", side = 1, line = 4.2, cex = 0.7)

```

**Interpretation:**

-   **Dark Blue Circles**: Indicate strong positive correlations. Large and dark blue circles show pairs of variables that tend to increase together.

-   **Dark Red Circles**: Suggest strong negative correlations. Large and dark red circles show pairs of variables that move in opposite directions; as one increases, the other tends to decrease.

-   **Size of Circles**: The larger the circle, whether blue or red, the stronger the correlation. The smaller the circle, the weaker the correlation.

-   **No Circle or Neutral Color**: Indicates a very weak or no linear correlation between the variables.

**Highly Correlated pairs:**

-   Apps vs Accept

-   Apps vs Enroll

-   Apps vs F.Undergraduate

-   Accept vs F.Undergraduate

-   Enroll vs F.Undergraduate

-   Accept vs Enroll

-   Top10perc vs Top25perc

------------------------------------------------------------------------

8.  **Checking for Class Bias**

```{r}
table(df$Private)

```

-   We observe that there is a class bias for the positive class. We'll rectify it.

------------------------------------------------------------------------

## 2. Split the data into a train and test set

-   Code sourced from 3a_LogisticRegression_Jan2024.rmd (Shapiro)
-   The code addresses the issue of class bias by under-sampling the positive class and then splits the dataset in the ration 70:30 for training and testing respectively.

```{r}
# Create Training Data 70:30 (there are alternatives to dataset split)
TRAINING_FRACTION_OF_DATASET <- 0.7
input_ones <- df[which(df$Private == "Yes"), ]  # all 'Yes' instances
input_zeros <- df[which(df$Private == "No"), ]  # all 'No' instances

set.seed(100)  # seed random generator for repeatability of samples
no_bias_row_number <- nrow(input_zeros)  # Use the number of 'No' instances as the base for balancing

# Ensure that the dataset is shuffled to avoid bias
input_ones <- input_ones[sample(nrow(input_ones)), ]
input_zeros <- input_zeros[sample(nrow(input_zeros)), ]

# Balance the dataset by undersampling the 'Yes' instances
input_ones_no_bias <- head(input_ones, no_bias_row_number)

# Split the balanced datasets into the training and test sets
rnd_row_indices <- sample(1:no_bias_row_number, size = floor(TRAINING_FRACTION_OF_DATASET * no_bias_row_number))
ones_training <- input_ones_no_bias[rnd_row_indices, ]  # 'Yes' for training - randomly extract
zeros_training <- input_zeros[rnd_row_indices, ]  # 'No' for training - match the number of 'Yes' instances
trainingData <- rbind(ones_training, zeros_training)  # Combine the 'Yes' and 'No' instances

# Create Test Data
test_ones <- input_ones_no_bias[-rnd_row_indices, ]
test_zeros <- input_zeros[-rnd_row_indices, ]
testData <- rbind(test_ones, test_zeros)  # Combine the remaining 'Yes' and 'No' instances
cat("Training dataset size is = [", dim(trainingData), "] testing is = [", dim(testData), "]\n")

# Verify proportions in the training dataset
cat("Training dataset table of Private. Proportions should be equal.\n") 
table(trainingData$Private)


```

------------------------------------------------------------------------

## 3. Fit logistic regression model.

-   Code sourced from Lab: Logistic Regression Video. (Frasca, n.d.)

```{r}
#Fitting a logistic regression model
model <- glm(Private ~., data = trainingData, family = binomial(link = "logit") )
summary(model)
```

**Interpretation:**

1.  **F.Undergrad**: At 0.05 significance level, The variable "**F.Undergrad"** is statistically significant with a p-value of 0.005360 i.e p-value \< 0.05, suggesting a significant association between the number of full-time undergraduates and the likelihood of a university being private.
2.  **Outstate**: The coefficient for **Outstate** is highly significant with a p-value of $1.82e-06$ i.e p-value \< 0.05 , implying a very strong association between out-of-state tuition and the likelihood of a university being private.
3.  **P.Undergrad**: The coefficient for **P.Undergrad** is significant with a p-value of 0.0174340 i.e p-value \< 0.05, indicating a significant relationship between the number of part-time undergraduates and the likelihood of a university being private.
4.  **Grad.Rate**: The coefficient for **Grad.Rate** is significant with a p-value of 0.0326800 i.e p-value \< 0.05, indicating a significant relationship between graduation rate and the likelihood of a university being private.

**Model Metrics:**

-   Generally, the null deviance and Residual deviance should not be equal and in the above result, we see that there's a difference between the two indicating that our model has performed well in terms of prediction.

------------------------------------------------------------------------

## 4. Create a confusion matrix and report the results of your model predictions on the train set. Interpret and discuss the confusion matrix. Which misclassifications are more damaging for the analysis, False Positives or False Negatives?

-   Code sourced from Lab: Logistic Regression Video. (Frasca, n.d.)

-   **Making predictions and creating confusion matrix for training dataset.**

```{r}
#Train set predictions
train_predicted_prob <- predict(model, newdata = trainingData, type = "response")
train_predicted_class <- as.factor(ifelse(train_predicted_prob>= 0.5, "Yes", "No"))

#Model Accuracy
# Create confusion matrix

train_confusion_matrix <- confusionMatrix(train_predicted_class, trainingData$Private, positive = 'Yes')
train_confusion_matrix

```

**Confusion Matrix:**

-   **True Negatives (TN)**: 142 cases were correctly predicted as "No" (public universities).

-   **True Positives (TP)**: 141 cases were correctly predicted as "Yes" (private universities).

-   **False Positives (FP)**: 7 cases were incorrectly predicted as "Yes" when they were actually "No".

-   **False Negatives (FN)**: 6 cases were incorrectly predicted as "No" when they were actually "Yes".

**Interpretation and Discussion:** The model demonstrates high performance in all metrics, with accuracy, sensitivity, and specificity all above 95%. The balanced accuracy also indicates that the model is equally good at identifying both classes, which is particularly important in the context of a balanced dataset.

**Misclassifications:**

-   **False Positives (FP)**: Public universities incorrectly identified as private. This could be damaging if, for example, policies or resources intended for public universities are misallocated to private universities based on the model's predictions.

-   **False Negatives (FN)**: Private universities incorrectly identified as public. This could be damaging in scenarios where private universities are supposed to receive certain benefits or are subject to regulations that would not be correctly applied due to the misclassification.

Which misclassification is more damaging depends on the context of the model's use. If the goal is to identify private universities for the provision of exclusive benefits or opportunities, then false negatives would be more damaging. Conversely, if the model is used to target public universities for specific interventions or support, then false positives would be more damaging.

------------------------------------------------------------------------

## 5. Report and interpret Accuracy, Precision, Recall, and Specificity metrics

**For the confusion matrix obtained in the previous step, below are the statistics:**

-   **Accuracy**: About 95.61% of predictions were correct.

-   **Positive Predictive Value (Precision)**: About 95.92% of predicted "Yes" cases were correct.

-   **Sensitivity (Recall or True Positive Rate)**: About 95.27% of actual "Yes" cases were correctly identified.

-   **Specificity (True Negative Rate)**: About 95.95% of actual "No" cases were correctly identified.

------------------------------------------------------------------------

## 6. Create a confusion matrix and report the results of your model for the test set. Compare the results with the train set and interpret.

-   Code sourced from Lab: Logistic Regression Video. (Frasca, n.d.)

-   **Making predictions and creating confusion matrix for test dataset.**

```{r}
#Train set predictions
test_predicted_prob <- predict(model, newdata = testData, type = "response")
test_predicted_class <- as.factor(ifelse(test_predicted_prob >= 0.5, "Yes", "No"))

#Model Accuracy
# Create confusion matrix
test_confusion_matrix <-confusionMatrix(test_predicted_class, testData$Private, positive = 'Yes')
test_confusion_matrix

```

**For the confusion matrix obtained above, below are the statistics:**

-   **Accuracy**: About 90.62% of predictions were correct.

-   **Positive Predictive Value (Precision)**: About 93.33% of predicted "Yes" cases were correct.

-   **Sensitivity (Recall or True Positive Rate)**: About 87.50% of actual "Yes" cases were correctly identified.

-   **Specificity (True Negative Rate)**: About 93.75% of actual "No" cases were correctly identified.

**Comparing the training and testing set**

```{r}
# Extract performance metrics for training set
# Extract performance metrics
train_accuracy <- train_confusion_matrix$overall['Accuracy']
train_sensitivity <- train_confusion_matrix$byClass['Sensitivity']
train_specificity <- train_confusion_matrix$byClass['Specificity']
train_precision <- train_confusion_matrix$byClass['Pos Pred Value']

test_accuracy <- test_confusion_matrix$overall['Accuracy']
test_sensitivity <- test_confusion_matrix$byClass['Sensitivity']
test_specificity <- test_confusion_matrix$byClass['Specificity']
test_precision <- test_confusion_matrix$byClass['Pos Pred Value']

# Create a data frame for visualization
comparison_df <- data.frame(
  Metric = rep(c("Accuracy", "Sensitivity", "Specificity", "Precision"), each = 2),
  Value = c(train_accuracy, test_accuracy, 
            train_sensitivity, test_sensitivity,
            train_specificity, test_specificity,
            train_precision, test_precision),
  Dataset = rep(c("Training", "Test"), 4)
)

# Above code sourced from (OpenAI,2024)

# Plot comparison of performance metrics
ggplot(comparison_df, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7)) +
  geom_text(aes(label = sprintf("%.2f%%", Value * 100)), 
            position = position_dodge(width = 0.7), vjust = -0.5, size = 3) +
  labs(title = "Figure 7: Comparison of Model Performance Metrics", x = "", y = "Metric Value", caption="Note. The above barchart shows the comparison of different performance metrics for Training and Test Dataset.") +
  theme_minimal() +
  scale_fill_brewer(palette = "Pastel1") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
theme(
        plot.title = element_text(hjust = 0),
        plot.caption = element_text(hjust = 0, size = 8))


```

**Interpretation of Figure 7:**

-   **Accuracy**: The model shows an accuracy of **95.61%** on the training set and **90.62%** on the test set. This suggests that the model is slightly more accurate when predicting data it was trained on compared to new, unseen data.

-   **Precision**: The precision is **95.30%** for the training set and higher at **88.24%** for the test set, indicating that a high percentage of the positive (private university) predictions made by the model are correct.

-   **Sensitivity (Recall)**: Sensitivity is **93.75%** for the training set and very close at **95.95%** for the test set, meaning the model is very good at correctly identifying actual positive cases in both datasets.

-   **Specificity**: Specificity is consistent across both datasets, with **95.27%** for the training set and **87.50%** for the test set, showing that the model is similarly effective at identifying actual negative cases (public universities) in both.

The model is performing well across all metrics, but it does perform slightly better on the training data than on the test data. This is a common occurrence, as models tend to perform better on data they have seen before. However, the differences are not large, indicating good generalization to new data.

------------------------------------------------------------------------

## 7. Plot and interpret the ROC curve.

-   Code sourced from Lab: Logistic Regression Video. (Frasca, n.d.)

```{r}
# Generate the ROC curve object
roc_curve <- roc(testData$Private, test_predicted_prob)

# Plot ROC curve
plot(roc_curve, main="Figure 6: ROC Curve",
     xlab="1-Specificity (False Positive Rate)",
     ylab="Sensitivity (True Positive Rate)",
     xlim = c(1, 0),
     col="blue")


```

**Interpretation:**

\
The ROC curve demonstrates **excellent model performance**, with a high true positive rate (sensitivity) and a low false positive rate (1 - specificity), as indicated by the curve's proximity to the top left corner. We should look for such pattern for ROC curve and make sure that the curve is not touching or hugging the diagonal line. The AUC is likely very high, suggesting strong discrimination between the positive and negative classes.

------------------------------------------------------------------------

## 8. Calculate and interpret the AUC.

-   Code sourced from Lab: Logistic Regression Video. (Frasca, n.d.)

```{r}
# AUC value
auc_value <- auc(roc_curve)
print(auc_value)

```

**Interpretation of AUC score:**

With an AUC of **0.9739**, our model has a very high discriminative ability to correctly classify the positive class (private universities) and the negative class (public universities). It means that there's a **97.39%** chance that the model will be able to distinguish between a randomly chosen positive instance and a negative instance.

------------------------------------------------------------------------

# Conclusion

Through meticulous exploratory data analysis, we observed distinct differences in out-of-state tuition, room and board costs, personal spending, and faculty qualifications between private and public universities. Our logistic regression model, trained on a balanced dataset, exhibited high accuracy, precision, sensitivity, and specificity, indicating strong predictive performance. On the training set, our model achieved an accuracy of **95.61%**, slightly outperforming the test set accuracy of **90.62%**. The model's precision was higher in the training set **(95.30%)** compared to the test set **(88.24%)**, while sensitivity and specificity remained consistent across both datasets. The ROC curve analysis revealed an AUC of **0.9739**, confirming the model's excellent capability to differentiate between private and public universities. In summary, our logistic regression model proved to be a robust tool for predicting the type of university, demonstrating potential utility in educational data analysis and policy-making contexts.

------------------------------------------------------------------------

# References

1.  Frasca. (n.d.). *Lab: Logistic Regression Video* [Video]. Panopto.

2.  *OpenAI. (2021). ChatGPT (Version 3.5). OpenAI.<https://chat.openai>.com/*

3.  Shapiro, V. *3a\_\_Logistic Regression - revised in Jan 2024* [.Rmd file].
