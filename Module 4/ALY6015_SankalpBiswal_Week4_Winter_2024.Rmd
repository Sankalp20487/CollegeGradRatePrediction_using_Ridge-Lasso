---
title: 'Module 4: Regularization'
author: "Sankalp Biswal"
date: "2024-02-02"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

# Introduction

In the field of machine learning, model regularization is a critical technique used to prevent overfitting, improve model generalization, and handle multicollinearity. This document explores the application of Ridge and LASSO regression --- two popular regularization methods --- and compares their performance to stepwise feature selection on the \`College\` dataset from the \`ISLR\` library. The aim is to identify which model provides the best fit for predicting the graduation rate (\`Grad.Rate\`) of colleges.

------------------------------------------------------------------------

## Importing Libraries

```{r}
list.of.packages <- c( "ggplot2", "pls", "Matrix", "caTools", "glmnet")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
require(glmnet)
require(caTools)
#tinytex::install_tinytex() 
library(MASS)  # For stepwise selection
library(caret)  # For RMSE calculation
library(ISLR)
library(knitr)
library(kableExtra)
```

## Importing Dataset

```{r}
df <- College
```

------------------------------------------------------------------------

# Ridge regression

## 1. Split the data into a train and test set

```{r}
set.seed(123) 
trainIndex <- sort(sample(x = nrow(df), size = nrow(df) * 0.7))
train <- df[trainIndex,]
test <- df[-trainIndex,]
```

## 2. Use the cv.glmnet function to estimate the lambda.min and lambda.1se values. Compare and discuss the values.

-   Code below has been sourced from 4b_Regularization.Rmd

```{r}
set.seed(123) 

lambda_seq <- 10^seq(2, -2, by = -.1)


train_x     <- model.matrix(Grad.Rate ~. , train)[,-1]
train_y      <- train$Grad.Rate
test_x     <- model.matrix(Grad.Rate ~. , test)[,-1]
test_y      <- test$Grad.Rate

# Standardize features for Ridge regression (OpenAI,2024)
train_x_scaled <- scale(train_x)
test_x_scaled <- scale(test_x, center = attr(train_x_scaled, "scaled:center"), scale = attr(train_x_scaled, "scaled:scale"))

# Finding best value of lamda using cross validation

cv_output  <- cv.glmnet(train_x_scaled, train_y, alpha = 0, lambda = lambda_seq)

lambda.min <- cv_output$lambda.min
lambda.1se <- cv_output$lambda.1se

cat("lambda.min =", lambda.min)
cat("\n")
cat("lamda.1se =", lambda.1se)

```

**`lambda.min`** (1.258925) represents the value of the regularization parameter that minimizes cross-validation error, offering the best predictive performance but potentially leading to a more complex model. **`lambda.1se`** (12.58925) is a more conservative choice, within one standard error of the minimum error, leading to a simpler, more interpretable model but possibly at the cost of slightly reduced predictive accuracy.

## 3. Plot the results from the cv.glmnet function and provide an interpretation. What does this plot tell us?

```{r}

plot(cv_output)

# Add a title
title("Figure 1: Cross-Validation Results", line = 3)

# Add vertical lines for lambda.min and lambda.1se
abline(v = log(cv_output$lambda.min), col = "red", lty = 2)
abline(v = log(cv_output$lambda.1se), col = "blue", lty = 2)

# Annotations with lambda.min and lambda.1se values
text(x = log(cv_output$lambda.min), y = par("usr")[4] * 0.8, labels = paste("lambda.min =", round(cv_output$lambda.min, 4)), pos = 4, col = "red")
text(x = log(cv_output$lambda.1se), y = par("usr")[4] * 0.7, labels = paste("lambda.1se =", round(cv_output$lambda.1se, 4)), pos = 3, col = "blue")

# Add a note at the bottom
mtext("Note: This plot shows the cross-validation results for lambda values", side = 1, line = 4, adj = 0, cex = 0.8)
```

**Figure 1:** This plot displays the cross-validation curve for different values of the regularization parameter, lambda (λ), plotted on a logarithmic scale (Log(λ)). The y-axis represents the mean squared error (MSE) from cross-validation.

**Interpretation:**

-   The model complexity decreases as we move to the right on the x-axis (increasing λ), meaning more regularization and potentially fewer predictors in the model.

-   The minimum point of the curve (near the left vertical line) suggests the best-performing model in terms of MSE with the least regularization.

-   As we move to larger values of λ, the error increases, which is typical as over-regularization can lead to underfitting. (OpenAI, 2024)

-   The **`lambda.1se`** rule (right vertical line) offers a more regularized model, which may be preferable if we are concerned about overfitting or if we value a simpler model with potentially fewer predictors.

In summary, this plot helps in selecting a value for λ that balances model accuracy and complexity. It provides a visual tool to choose between the most accurate model (**`lambda.min`**) and a more regularized, simpler model (**`lambda.1se`**) that might perform slightly worse but could generalize better to unseen data.

## 4. Fit a Ridge regression model against the training set and report on the coefficients. Is there anything interesting?

### a. Ridge regression with lambda = lambda.min

```{r}
# Fit Ridge regression model with lambda.min
ridge_model <- glmnet(train_x_scaled, train_y, alpha = 0, lambda = lambda.min)

# Display coefficients
coef(ridge_model)

```

**Interpretation of coefficients:**

-   **(Intercept)**: The intercept is approximately 65.55, which suggests that the expected graduation rate is 65.55% when all other predictor variables are held at zero. This may not be a meaningful interpretation if zero is not within the range of plausible values for the predictors.

-   **PrivateYes**: The coefficient for PrivateYes is around 1.93, indicating that, on average, private colleges have a graduation rate that is 1.93 percentage points higher than public colleges, holding all else constant.

-   **Apps**: A coefficient of 3.11 for Apps suggests that an increase in the number of applications is associated with a higher graduation rate. This might reflect that colleges with more applicants can be more selective, which may correlate with higher graduation rates.

-   **Accept**: The Accept coefficient is smaller (0.26) compared to Apps, indicating a less pronounced effect of the number of acceptances on the graduation rate.

-   **Enroll**: There's a negative coefficient for Enroll (-0.36), suggesting that as enrollment numbers increase, the graduation rate tends to decrease slightly, perhaps indicating the challenges of maintaining high graduation rates with larger student bodies.

-   **Top10perc**: The positive coefficient for Top10perc (1.14) implies that colleges with a higher percentage of students from the top 10% of their high school classes tend to have higher graduation rates.

-   **Top25perc**: Similarly, Top25perc has a positive coefficient (2.92), reinforcing that having students from the top of their high school classes is associated with higher graduation rates.

-   **F.Undergrad and P.Undergrad**: Both full-time (F.Undergrad) and part-time (P.Undergrad) undergraduate coefficients are negative, suggesting that a larger undergraduate population, whether full-time or part-time, is associated with a lower graduation rate.

-   **Outstate**: The positive coefficient for Outstate (2.64) could suggest that colleges with higher tuition for out-of-state students tend to have higher graduation rates. This might reflect that institutions that can command higher tuition fees may have more resources, which could contribute to higher graduation rates.

-   **Room.Board**: The positive coefficient for Room.Board (2.28) suggests that colleges with more expensive room and board charges may also have higher graduation rates, potentially for similar reasons as the Outstate variable.

-   **Books, Personal, PhD, Terminal**: These coefficients reflect the relationship between graduation rates and the cost of books, personal expenses, and the faculty's educational level (PhD, Terminal degrees). The negative coefficient for Books and Personal expenses could be due to the financial burden on students, while the positive coefficients for PhD and Terminal may indicate that a more qualified faculty correlates with higher graduation rates.

-   **S.F.Ratio and perc.alumni**: The student-faculty ratio (S.F.Ratio) has a negative coefficient, suggesting that a higher ratio might negatively impact graduation rates. In contrast, the percentage of alumni who donate (perc.alumni) has a positive coefficient, indicating that a higher engagement of alumni through donations is associated with higher graduation rates.

-   **Expend**: The negative coefficient for Expend is intriguing, as it suggests that higher spending per student is associated with lower graduation rates, which is counterintuitive. This may require further investigation, as one might expect that more expenditure per student would typically lead to better resources and higher graduation rates.

### b. Ridge regression with lambda = lambda.1se

```{r}
# Fit Ridge regression model with lambda.min
ridge_model_1se <- glmnet(train_x_scaled, train_y, alpha = 0, lambda = lambda.1se)

# Display coefficients
coef(ridge_model)
```

**Interpretation of Coefficients:**

-   **(Intercept):** The intercept is approximately 65.55, suggesting that the expected value of the dependent variable (e.g., graduation rate, school ranking) is 65.55 when all predictor variables are held at zero. This baseline value might not be practically interpretable if zero is outside the plausible range for the predictors.

-   **PrivateYes:** The coefficient for PrivateYes is around 1.52, indicating that private colleges, on average, have a 1.52 unit higher value on the dependent variable compared to public colleges, holding all else constant. This could suggest advantages such as higher graduation rates or other positive outcomes associated with private colleges.

-   **Apps:** A coefficient of 1.05 for Apps suggests that an increase in the number of applications is associated with a slight increase in the dependent variable. This might indicate that more popular or selective colleges, receiving more applications, are correlated with better outcomes.

-   **Accept:** With a coefficient of 0.53 for Accept, this suggests a modest positive effect of the number of acceptances on the dependent variable, possibly indicating that colleges with higher acceptance rates may see slightly better outcomes in the measured variable.

-   **Enroll:** The positive coefficient for Enroll (0.04) is quite small, suggesting a very slight increase in the dependent variable with higher enrollment numbers, which might indicate that larger student bodies do not significantly impact the measured outcome.

-   **Top10perc:** The coefficient of 1.52 for Top10perc suggests that colleges with a higher percentage of students from the top 10% of their high school classes tend to have significantly better outcomes on the dependent variable, possibly reflecting higher academic standards or success rates.

-   **Top25perc:** Similarly, Top25perc has a positive coefficient of 1.95, further indicating that institutions with more academically distinguished students tend to perform better on the dependent variable.

-   **F.Undergrad:** The negative coefficient for F.Undergrad (-0.19) suggests a slight decrease in the dependent variable with larger full-time undergraduate populations, possibly reflecting the challenges of maintaining quality or performance with larger student bodies.

-   **P.Undergrad:** The coefficient of -1.41 for P.Undergrad indicates that a higher number of part-time undergraduates is associated with a notable decrease in the dependent variable, potentially reflecting challenges in achieving positive outcomes with larger part-time student populations.

-   **Outstate:** The coefficient of 2.02 for Outstate suggests that institutions with higher out-of-state tuition rates are associated with better outcomes on the dependent variable, possibly indicating better resources or perceived value.

-   **Room.Board:** With a coefficient of 1.69, higher costs for room and board are associated with better outcomes on the dependent variable, potentially tied to the quality of campus facilities or student life.

-   **Books:** The negative coefficient for Books (-0.10) suggests a slight decrease in the dependent variable with higher book costs, possibly due to the financial burden on students.

-   **Personal:** A coefficient of -1.58 for Personal indicates that higher personal expenses for students are associated with a notable decrease in the dependent variable, likely due to financial burdens.

-   **PhD:** The positive coefficient (0.74) for PhD suggests that institutions with a higher percentage of faculty holding PhD degrees are associated with better outcomes on the dependent variable.

-   **Terminal:** The very small negative coefficient for Terminal (-0.03) suggests a negligible decrease in the dependent variable with a higher percentage of faculty holding terminal degrees, which may require further investigation.

-   **S.F.Ratio:** The negative coefficient (-0.68) for S.F.Ratio suggests that a higher student-faculty ratio might be detrimental to the dependent variable, possibly indicating lower academic support or quality of education.

-   **perc.alumni:** The positive coefficient (2.46) for perc.alumni suggests that higher alumni donation rates are strongly associated with better outcomes on the dependent variable, indicating stronger alumni networks or financial support.

-   **Expend:** The small negative coefficient for Expend (-0.08) suggests a slight decrease in the dependent variable with higher spending per student. This is counterintuitive and might suggest inefficiencies or misallocation of resources, warranting further investigation.

### Q. Did I find anything interesting?

A. The most interesting point in the coefficients is the negative coefficient for **`Expend`**. This suggests that higher spending per student is associated with lower graduation rates, which is counterintuitive. Typically, one would expect that increased expenditure on students would lead to better educational outcomes, including higher graduation rates due to improved resources, facilities, and potentially more support services.

## 5. Determine the performance of the fit model against the training set by calculating the root mean square error (RMSE).

```{r}
# Predict on the training set for different values of lambda

train_y_pred <- predict(ridge_model, s = lambda.min, newx = train_x_scaled)
train_y_pred_1se <- predict(ridge_model_1se, s = lambda.1se, newx = train_x_scaled)

# Calculate RMSE using lambda = lambda.min
rmse_train <- sqrt(mean((train_y - train_y_pred)^2))

# Calculate RMSE using lambda = lambda.1se
rmse_train_1se <- sqrt(mean((train_y - train_y_pred_1se)^2))


rmse_table_train_ridge <- data.frame(
  Lambda = c("Lambda.min", "Lambda.1se"),
  RMSE = c(rmse_train, rmse_train_1se)
)
kable(rmse_table_train_ridge, caption = "Table 1: RMSE Values for Training Sets for Ridge Regression")%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = T) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(1:2, width = "6em") 


```

## 6. Determine the performance of the fit model against the test set by calculating the root mean square error (RMSE). Is your model overfit?

```{r}
# Predict on the test set using two different lambda's
test_y_pred <- predict(ridge_model, s = lambda.min, newx = test_x_scaled)
test_y_pred_1se <- predict(ridge_model_1se, s = lambda.1se, newx = test_x_scaled)

# Calculate RMSE using lambda.min
rmse_test_ridge <- sqrt(mean((test_y - test_y_pred)^2))


# Calculate RMSE using lambda.1se
rmse_test_1se_ridge <- sqrt(mean((test_y - test_y_pred_1se)^2))

rmse_table_test_ridge <- data.frame(
  Lambda = c("Lambda.min", "Lambda.1se"),
  RMSE = c(rmse_test_ridge, rmse_test_1se_ridge)
)
kable(rmse_table_test_ridge, caption = "Table 2: RMSE Values for Testing Sets for Ridge Regression")%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = T) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(1:2, width = "6em") 


```

### Q. Is your model overfit?

```{r}
# Create a data frame to store RMSE values
rmse_table <- data.frame(
  Dataset = c("Training", "Testing", "Difference"),
  RMSE = c(rmse_train, rmse_test_ridge, rmse_test_ridge - rmse_train),
   RMSE_lambda_1se = c(rmse_train_1se, rmse_test_1se_ridge, rmse_test_1se_ridge - rmse_train_1se)
)

kable(rmse_table, caption = "Table 3: RMSE Values for Training and Testing Sets")%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = T)%>%
  column_spec(1, bold = TRUE) %>%
  column_spec(1:3, width = "6em") 

```

**Interpretation:**

A model overfits if there's a significant difference between the RMSE value of the train and test set (OpenAI,2024). We'll consider model with `lambda = lambda.1se`, since the difference is lower between train and test RMSE. The model does not appear to be overfitting in our case, since the RMSE for the test set (12.9379) is only slightly higher than the RMSE for the training set (12.9276). This indicates that the model generalizes reasonably well to unseen data.

------------------------------------------------------------------------

# **Lasso regression**

## 7. Use the cv.glmnet function to estimate the lambda.min and lambda.1se values. Compare and discuss the values.

-   Code has been sourced from 4b_Regularization.Rmd

```{r}
set.seed(123) 

lambda_seq <- 10^seq(2, -2, by = -.1)


train_x     <- model.matrix(Grad.Rate ~. , train)[,-1]
train_y      <- train$Grad.Rate
test_x     <- model.matrix(Grad.Rate ~. , test)[,-1]
test_y      <- test$Grad.Rate

# Standardize features for Lasso regression (OpenAI,2024)
train_x_scaled <- scale(train_x)
test_x_scaled <- scale(test_x, center = attr(train_x_scaled, "scaled:center"), scale = attr(train_x_scaled, "scaled:scale"))

# Finding best value of lambda using cross validation

cv_output  <- cv.glmnet(train_x_scaled, train_y, alpha = 1, lambda = lambda_seq)

lambda.min <- cv_output$lambda.min
lambda.1se <- cv_output$lambda.1se

cat("lambda.min =", lambda.min)
cat("\n")
cat("lamda.1se =", lambda.1se)


```

**`lambda.min`** (0.1258925) represents the value of the regularization parameter that minimizes cross-validation error, offering the best predictive performance but potentially leading to a more complex model. **`lambda.1se`** (1) is a more conservative choice, within one standard error of the minimum error, leading to a simpler, more interpretable model but possibly at the cost of slightly reduced predictive accuracy.

## 8. Plot the results from the cv.glmnet function and provide an interpretation. What does this plot tell us?

```{r}
plot(cv_output) 

# Add a title
title("Figure 2: Cross-Validation Results", line = 3)

# Add vertical lines for lambda.min and lambda.1se
abline(v = log(cv_output$lambda.min), col = "red", lty = 2)
abline(v = log(cv_output$lambda.1se), col = "blue", lty = 2)

# Annotations with lambda.min and lambda.1se values
text(x = log(cv_output$lambda.min), y = par("usr")[4] * 0.8, labels = paste("lambda.min =", round(cv_output$lambda.min, 4)), pos = 4, col = "red")
text(x = log(cv_output$lambda.1se), y = par("usr")[4] * 0.7, labels = paste("lambda.1se =", round(cv_output$lambda.1se, 4)), pos = 4, col = "blue")

# Add a note at the bottom
mtext("Note: This plot shows the cross-validation results for lambda values, with annotations for lambda.min and lambda.1se.", side = 1, line = 4, adj = 0, cex = 0.8)

```

**Figure 2:** This plot displays the cross-validation curve for different values of the regularization parameter, lambda (λ), plotted on a logarithmic scale (Log(λ)). The y-axis represents the mean squared error (MSE) from cross-validation.

**Interpretation:**

-   The model complexity decreases as we move to the right on the x-axis (increasing λ), meaning more regularization and potentially fewer predictors in the model.

-   The minimum point of the curve (near the left vertical line) suggests the best-performing model in terms of MSE with the least regularization.

-   As we move to larger values of λ, the error increases, which is typical as over-regularization can lead to underfitting.

-   The **`lambda.1se (1)`** rule (right vertical line) offers a more regularized model, which may be preferable if we are concerned about overfitting or if we value a simpler model with potentially fewer predictors.

In summary, this plot helps in selecting a value for λ that balances model accuracy and complexity. It provides a visual tool to choose between the most accurate model (**`lambda.min`**) and a more regularized, simpler model (**`lambda.1se`**) that might perform slightly worse but could generalize better to unseen data.

## 9. Fit a Lasso regression model against the training set and report on the coefficients. Is there anything interesting?

### a. Lasso regression with lambda = lambda.min

```{r}
# Fit Lasso regression model with lambda.min
Lasso_model <- glmnet(train_x_scaled, train_y, alpha = 1, lambda = lambda.min)

# Display coefficients
coef(Lasso_model)

```

**Interpretation of coefficients:**

-   **(Intercept)**: The intercept is approximately 65.55, suggesting that the expected value of the dependent variable graduation rate) is 65.55 when all other predictor variables are held at zero. I

-   **PrivateYes**: The coefficient for PrivateYes is around 2.08, indicating that, holding all else constant, private institutions are associated with an approximately 2.08 unit increase in the dependent variable compared to public institutions. This could imply higher graduation rates, better performance metrics, or other positive outcomes associated with private institutions.

-   **Apps**: A coefficient of 3.48 for Apps suggests that an increase in the number of applications is associated with an increase in the dependent variable. This could indicate that institutions with more applicants have higher selectivity or popularity, potentially correlating with positive outcomes like higher graduation rates or rankings.

-   **Enroll**: The negative coefficient for Enroll (-0.35) suggests that as enrollment numbers increase, there may be a slight decrease in the dependent variable, possibly indicating challenges in maintaining quality or performance metrics with larger student bodies.

-   **Top10perc**: The positive coefficient (0.29) for Top10perc implies that institutions with a higher percentage of students from the top 10% of their high school classes tend to score higher on the dependent variable, possibly reflecting better academic outcomes or prestige.

-   **Top25perc**: Similarly, Top25perc has a positive coefficient (3.52), suggesting that having more students from the top 25% of their high school classes is beneficial for the institution's performance on the dependent variable.

-   **P.Undergrad**: The negative coefficient for P.Undergrad (-1.99) indicates that a higher number of part-time undergraduates is associated with a decrease in the dependent variable, potentially reflecting challenges in achieving positive outcomes with a larger part-time student population.

-   **Outstate**: The coefficient of 2.78 for Outstate might suggest that institutions with higher out-of-state tuition rates are associated with higher values of the dependent variable, possibly indicating better resources or perceived value.

-   **Room.Board**: The coefficient of 2.28 for Room.Board suggests that higher costs for room and board are associated with an increase in the dependent variable, which could be tied to the quality of campus facilities or student life.

-   **Personal**: A negative coefficient for Personal (-1.97) might indicate that higher personal expenses for students are associated with lower values of the dependent variable, possibly due to financial burdens.

-   **PhD**: The positive coefficient (1.63) for PhD suggests that institutions with a higher percentage of faculty holding PhD degrees are associated with better outcomes on the dependent variable.

-   **Terminal**: The negative coefficient for Terminal (-1.53) is intriguing and might suggest that a higher percentage of faculty with terminal degrees is not necessarily associated with better outcomes on the dependent variable, which could warrant further investigation.

-   **S.F.Ratio**: The negative coefficient (-0.61) for S.F.Ratio suggests that a higher student-faculty ratio might be detrimental to the dependent variable, possibly indicating lower academic support or quality of education.

-   **perc.alumni**: The positive coefficient (3.98) for perc.alumni suggests that higher alumni donation rates are associated with better outcomes on the dependent variable, potentially indicating stronger alumni networks or financial support.

-   **Expend**: The negative coefficient for Expend (-2.02) is counterintuitive, as it suggests that higher spending per student is associated with lower values of the dependent variable. This could be due to inefficiencies or misallocation of resources and merits further investigation.

### Q. Did any coefficients reduce to zero? If so, which ones?

**A. Coefficients reduced to zero**: Variables like **`Accept`**, **`F.Undergrad`**, and **`Books`** have coefficients that were reduced to zero, indicating they were not significant predictors in the model given the Lasso penalty.

## b. Lasso regression with lambda = lambda.1se

```{r}
Lasso_model_1se <- glmnet(train_x_scaled, train_y, alpha = 1, lambda = lambda.1se)
coef(Lasso_model_1se)
```

**Interpretation of Coefficients:**

-   **(Intercept):** The intercept is approximately 65.55, indicating that the expected value of the dependent variable is 65.55 when all other predictor variables are held at zero. This serves as a baseline against which the effects of the predictors are measured.

-   **PrivateYes:** The coefficient for PrivateYes is around 0.81, suggesting that, all else being equal, private institutions are associated with an approximately 0.81 unit increase in the dependent variable compared to public institutions. This might indicate a slight advantage in outcomes like graduation rates or academic performance for private institutions.

-   **Apps:** A coefficient of 0.90 for Apps implies that an increase in the number of applications is associated with a modest increase in the dependent variable. This could reflect that more popular institutions, which receive more applications, tend to have higher outcomes in the measured variable.

-   **Top10perc:** The coefficient of 0.19 for Top10perc suggests that institutions with a higher percentage of students from the top 10% of their high school classes see a marginal increase in the dependent variable, possibly indicating a positive impact on academic outcomes or institutional prestige.

-   **Top25perc:** With a coefficient of 3.37, Top25perc has a significant positive impact, indicating that having a greater proportion of top students correlates strongly with the dependent variable, which could reflect higher academic standards or success rates.

-   **P.Undergrad:** The coefficient of -1.21 for P.Undergrad indicates that a higher number of part-time undergraduates is associated with a decrease in the dependent variable, possibly suggesting challenges in achieving positive outcomes with larger part-time student populations.

-   **Outstate:** The coefficient of 3.04 for Outstate suggests that institutions with higher out-of-state tuition rates are associated with higher values of the dependent variable, possibly indicating better resources, reputation, or perceived value.

-   **Room.Board:** With a coefficient of 1.82, higher costs for room and board are associated with an increase in the dependent variable, potentially tied to the quality of campus facilities or student life.

-   **Personal:** A coefficient of -1.42 for Personal suggests that higher personal expenses for students are associated with a slight decrease in the dependent variable, possibly due to the financial burden on students.

-   **perc.alumni:** The coefficient of 3.36 for perc.alumni suggests that higher alumni donation rates are strongly associated with better outcomes on the dependent variable, indicating stronger alumni networks or financial support.

### Q. Did any coefficients reduce to zero? If so, which ones?

**A. Coefficients Reduced to Zero:** The model has effectively shrunk the coefficients for **`Accept`**, **`Enroll`**, **`F.Undergrad`**, **`Books`**, **`PhD`**, **`Terminal`**, **`S.F.Ratio`**, and **`Expend`** to zero, indicating these variables were not significant predictors in the presence of the other variables and the L1 penalty applied by the Lasso regression.

## 10. Determine the performance of the fit model against the training set by calculating the root mean square error (RMSE).

```{r}
# Predict on the training set
train_y_pred <- predict(Lasso_model, s = lambda.min, newx = train_x_scaled)
train_y_pred_1se <- predict(Lasso_model_1se, s = lambda.1se, newx = train_x_scaled)

# Calculate RMSE using lambda = lambda.min
rmse_train <- sqrt(mean((train_y - train_y_pred)^2))

# Calculate RMSE using lambda = lambda.1se
rmse_train_1se <- sqrt(mean((train_y - train_y_pred_1se)^2))


rmse_table_train_lasso <- data.frame(
  Lambda = c("Lambda.min", "Lambda.1se"),
  RMSE = c(rmse_train, rmse_train_1se)
)
kable(rmse_table_train_lasso, caption = "Table 4: RMSE Values for Training Sets for Lasso Regression")%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = T) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(1:2, width = "6em") 

```

## 11. Determine the performance of the fit model against the test set by calculating the root mean square error (RMSE). Is your model overfit?

```{r}
# Predict on the test set using two different lambda's
test_y_pred <- predict(Lasso_model, s = lambda.min, newx = test_x_scaled)
test_y_pred_1se <- predict(Lasso_model_1se, s = lambda.1se, newx = test_x_scaled)

# Calculate RMSE using lambda.min
rmse_test_lasso <- sqrt(mean((test_y - test_y_pred)^2))

# Calculate RMSE using lambda.1se
rmse_test_1se_lasso <- sqrt(mean((test_y - test_y_pred_1se)^2))


rmse_table_test_lasso <- data.frame(
  Lambda = c("Lambda.min", "Lambda.1se"),
  RMSE = c(rmse_test_lasso, rmse_test_1se_lasso)
)
kable(rmse_table_test_lasso, caption = "Table 5: RMSE Values for Testing Sets for Lasso Regression")%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = T) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(1:2, width = "6em") 

```

### Q. Is your model overfit?

```{r}
# Create a data frame to store RMSE values
rmse_table <- data.frame(
  Dataset = c("Training", "Testing", "Difference"),
  RMSE = c(rmse_train, rmse_test_lasso, rmse_test_lasso - rmse_train),
   RMSE_lambda_1se = c(rmse_train_1se, rmse_test_1se_lasso, rmse_test_1se_lasso - rmse_train_1se)
)

kable(rmse_table, caption = "Table 6: RMSE Values for Training and Testing Sets for Lasso Regression")%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = T) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(1:3, width = "6em") 

```

**Interpretation:**

A model overfits if there's a significant difference between the RMSE value of the train and test set (OpenAI,2024). We'll select `lambda= lambda.1se` , Since the difference is lower. The model does not appear to be overfitting in our case, since the RMSE for the test set (12.9146) is only slightly higher than the RMSE for the training set (12.8817). This indicates that the model generalizes reasonably well to unseen data.

## 12. Which model performed better and why? Is that what you expected?

```{r}
# Load the necessary library
library(ggplot2)

# Create a data frame containing the RMSE values
rmse_values <- data.frame(
  Model = rep(c("Ridge", "Lasso"), each = 2),
  Dataset = rep(c("Training", "Testing"), 2),
  RMSE = c(12.9276, 12.9379, 12.8817, 12.9146)
)

# Plotting graph for comparison of Lasso and Ridge RMSE values (OpenAI,2024)
ggplot(rmse_values, aes(x = Dataset, y = RMSE, group = Model, color = Model)) +
  geom_line(aes(linetype = Model), size = 1) + # Add lines
  geom_point(size = 3) + # Add points
  geom_text(aes(label = round(RMSE, 2)), vjust = 0, hjust = 1.5) + # Add text annotations
  theme_minimal() +
  labs(title = "Figure 3: Comparison of RMSE Values for Ridge and Lasso Regression",
       x = "",
       y = "RMSE",
       color = "Model",
       linetype = "Model",
       caption = "Note: RMSE values are rounded to two decimal places.") + 
  scale_color_manual(values = c("Ridge" = "blue", "Lasso" = "red")) + # Custom colors
  theme(legend.position = "top",
        plot.caption = element_text(hjust = 0)) # Align caption to the left



```

**Interpretation:**

The graph shows the RMSE values for both Ridge and Lasso regression models on the training and testing sets. Here's the interpretation:

-   The RMSE values for the training set are lower for **Lasso** (red line) than for **Ridge** (blue line), indicating that Lasso had a slightly better fit on the training data.

-   The RMSE values for the testing set are very close for both models, but **Lasso** still has a marginally lower RMSE, suggesting it generalized slightly better to the testing data.

-   The difference between the training and testing RMSE for both models is minimal, but it's slightly smaller for **Lasso**, indicating that **Lasso** may have a slight edge in terms of generalization.

### **Q. Which Model Performed Better?**

A. Based on this graph, the **Lasso** regression model performed slightly better than the **Ridge** regression model. This conclusion is drawn from the fact that **Lasso** has both a lower RMSE on the training data and a lower (or at least very close) RMSE on the testing data, implying it's fitting the data well without overfitting.

### **Q. Is that what you expected?**

A. Based on the two regularization, I expected the Lasso model to perform better due to it's ability to select features which lead to elimination of redundant variables. Indeed, the Lasso model performed better in this case.

------------------------------------------------------------------------

## 13. Refer to ALY6015_Feature_Selection_R.pdf document for how to perform stepwise selection and then fit a model. Did this model perform better or as well as Ridge regression or LASSO? Which method do you prefer and why?

-   Some parts have been sourced from ALY6015_Feature_Selection_R.pdf

```{r}
# Fit a full model with all predictors for the stepwise selection
full_model <- lm(Grad.Rate ~ ., data = train)

# Perform stepwise selection using both directions (forward and backward)
stepwise_model <- step(full_model, direction = "both", trace = 0)

# Above code sourced from Chatgpt

# Summary of the stepwise model
summary(stepwise_model)

# Make predictions on the test set
predictions_stepwise <- predict(stepwise_model, newdata = test)

# Calculate RMSE for stepwise model
rmse_stepwise <- sqrt(mean((predictions_stepwise - test$Grad.Rate) ^ 2))
rmse_stepwise

# Compare test RMSE values
rmse_comparison <- data.frame(
  Model = c("Stepwise", "Ridge", "LASSO"),
  RMSE = c(rmse_stepwise, rmse_test_1se_ridge, rmse_test_1se_lasso)
)

# Print the RMSE comparison
print(rmse_comparison)

# Decide on the preferred method
preferred_method <- ifelse(rmse_stepwise < rmse_test_1se_ridge & rmse_stepwise < rmse_test_1se_lasso, "Stepwise", ifelse(rmse_test_1se_ridge < rmse_test_1se_lasso, "Ridge", "LASSO"))
cat("Preferred method based on RMSE:", preferred_method)


```

**Interpretation:**

The preferred model would be the **LASSO Regression** because it has the lowest RMSE value of 12.91464. A lower RMSE value indicates a model that better fits the dataset, as it means the predicted values are closer to the actual values on average.

The preference for **LASSO** over **Ridge** and **Stepwise Regression** is because it not only provides the best predictive accuracy among the three but also because **LASSO** has the advantage of performing feature selection. This can result in a simpler model by potentially reducing the coefficients of less important predictors to zero, which can be particularly useful if there are many predictors and some are not relevant.

------------------------------------------------------------------------

# **Conclusion**

In comparing the RMSE scores, LASSO regression achieved the best performance with the lowest RMSE, indicating it is the most suitable model among the ones tested for predicting the graduation rate of colleges in our dataset. The strength of LASSO regression lies in its ability to perform feature selection, thus simplifying the model by eliminating non-informative predictors and reducing the likelihood of overfitting.

While Ridge regression also performed well, the slight difference in RMSE indicates that the penalty on coefficients it applies does not translate into a substantial improvement in this particular case. Stepwise feature selection, despite being a popular traditional method, did not outperform the regularization techniques.

------------------------------------------------------------------------

# References

1.  Frasca. (n.d.). *Lab: Regularization Video* [Video]. Panopto.

2.  *OpenAI. (2021). ChatGPT (Version 3.5). OpenAI.<https://chat.openai>.com/*

3.  Shapiro, V. *4b\_\_Regularization - revised in Jan 2024* [.Rmd file].

4.  Northeastern University. (n.d.). *ALY6015 Feature Selection R* [PDF file].
